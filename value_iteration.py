# -*- coding: utf-8 -*-
"""value_iteration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f5URMEEx6jnoIbS89WdwK7Ux4egHLgMB
"""

import numpy as np

# Define states representing the car's location
states = ["Top", "Rolling", "Bottom"]

# Define actions the car can take
actions = ["Drive", "DontDrive"]  # Minor correction for consistency

# Define transition probabilities and rewards as a dictionary
# The key is the current state, the value is another dictionary with actions as keys.
# Each action maps to a list of tuples (probability, next_state, reward)
transition_probabilities = {
    "Top": {
        "Drive": [(0.5, "Top", 2), (0.5, "Rolling", 2)],
        "DontDrive": [(0.5, "Top", 3), (0.5, "Rolling", 1)],
    },
    "Rolling": {
        "Drive": [(0.3, "Top", 2), (0.4, "Rolling", 1.5), (0.3, "Bottom", 0.5)],
        "DontDrive": [(1, "Bottom", 1)],
    },
    "Bottom": {
        "Drive": [(0.5, "Top", 2), (0.5, "Bottom", 2)],
        "DontDrive": [(1, "Bottom", 1)],
    },
}

# Discount factor for future rewards (gamma)
discount_factor = 0.9

# Initial values for each state (all zeros)
state_values = {state: 0 for state in states}

# Value iteration algorithm

# Epsilon for convergence check (very small value)
convergence_threshold = 1e-6

# Maximum number of iterations
max_iterations = 1000

for iteration in range(max_iterations):
    # Dictionary to store new state values after each iteration
    new_state_values = {}

    for state in states:
        # List to store action values for the current state
        action_values = []
        for action in actions:
            # Get transition probabilities and rewards for the current state and action
            transitions = transition_probabilities[state][action]

            # Calculate the expected value for taking this action
            action_value = sum(
                prob * (reward + discount_factor * state_values[next_state])
                for prob, next_state, reward in transitions
            )
            action_values.append(action_value)

        # Update the state value with the maximum action value
        new_state_values[state] = max(action_values)

    # Check for convergence (max difference in state values between iterations)
    max_difference = max(
        abs(new_state_values[state] - state_values[state]) for state in states
    )
    if max_difference < convergence_threshold:
        break

    # Update state values for the next iteration
    state_values = new_state_values

# Calculate optimal policy (action with highest expected value for each state)
optimal_policy = {
    state: actions[np.argmax([sum(prob * (reward + discount_factor * state_values[next_state]) for prob, next_state, reward in transition_probabilities[state][action]) for action in actions])]
    for state in states
}

# Print results

print("\nOptimal Values:")
for state in states:
    print(f"{state}: {state_values[state]:.12f}")

print("\nOptimal Policy:")
for state in states:
    print(f"Policy for state {state}: {optimal_policy[state]}")

