# -*- coding: utf-8 -*-
"""policy_iteration_with_deterministic_policy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BrSy2krB0E9oInqsWRfiCEpd7j4AUGjj
"""

import numpy as np


def policy_iteration(initial_policy, states, actions, transition_probabilities, discount_factor, convergence_threshold=1e-6, max_iterations=1000):

  # Initialize with the provided initial policy
  policy = initial_policy.copy()
  state_values = {state: 0 for state in states}

  for iteration in range(max_iterations):
    # Policy evaluation step
    is_converged = True
    for state in states:
      action_value = sum(
          prob * (reward + discount_factor * state_values[next_state])
          for prob, next_state, reward in transition_probabilities[state][policy[state]]
      )
      if abs(action_value - state_values[state]) > convergence_threshold:
        is_converged = False
      state_values[state] = action_value

    # Check for convergence after evaluation
    if is_converged:
      break

    # Policy improvement step
    for state in states:
      best_action = None
      best_value = float("-inf")
      for action in actions:
        action_value = sum(
            prob * (reward + discount_factor * state_values[next_state])
            for prob, next_state, reward in transition_probabilities[state][action]
        )
        if action_value > best_value:
          best_value = action_value
          best_action = action
      policy[state] = best_action

  return policy, state_values


# Define states, actions, and transition probabilities (same as before)
states = ["Top", "Rolling", "Bottom"]
actions = ["Drive", "DontDrive"]
transition_probabilities = {
    "Top": {
        "Drive": [(0.5, "Top", 2), (0.5, "Rolling", 2)],
        "DontDrive": [(0.5, "Top", 3), (0.5, "Rolling", 1)],
    },
    "Rolling": {
        "Drive": [(0.3, "Top", 2), (0.4, "Rolling", 1.5), (0.3, "Bottom", 0.5)],
        "DontDrive": [(1, "Bottom", 1)],
    },
    "Bottom": {
        "Drive": [(0.5, "Top", 2), (0.5, "Bottom", 2)],
        "DontDrive": [(1, "Bottom", 1)],
    },
}

# Discount factor
discount_factor = 0.9

# Run policy iteration with a random initial policy (all actions have equal probability)
random_policy = {state: np.random.choice(actions) for state in states}
optimal_policy_random, optimal_values_random = policy_iteration(random_policy, states, actions, transition_probabilities, discount_factor)

# Run policy iteration with a deterministic initial policy (never drive)
deterministic_policy = {state: "DontDrive" for state in states}
optimal_policy_deterministic, optimal_values_deterministic = policy_iteration(deterministic_policy, states, actions, transition_probabilities, discount_factor)

# Print results

print("\nResults with Random Initial Policy:")
print("Optimal Values:")
for state in states:
    print(f"{state}: {optimal_values_random[state]:.12f}")
print("\nOptimal Policy:")
for state in states:
    print(f"Policy for state {state}: {optimal_policy_random[state]}")

print("\n\nResults with Deterministic Initial Policy (Never Drive):")
print("Optimal Values:")
for state in states:
    print(f"{state}: {optimal_values_deterministic[state]:.12f}")
print("\nOptimal Policy:")
for state in states:
    print(f"Policy for state {state}: {optimal_policy_deterministic[state]}")

