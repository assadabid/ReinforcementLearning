# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mlf7YE0cS3im1c6Wvr6CoArS92iFxqv9
"""

import gym as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import matplotlib.pyplot as plt

# Define the Deep Q-Network (DQN) class
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        # First fully connected layer with input dimension and 128 output neurons
        self.fc1 = nn.Linear(input_dim, 128)
        # Second fully connected layer with 128 input and 128 output neurons
        self.fc2 = nn.Linear(128, 128)
        # Third fully connected layer with 128 input neurons and output dimension
        self.fc3 = nn.Linear(128, output_dim)

    def forward(self, x):
        # Apply ReLU activation function to the output of the first layer
        x = torch.relu(self.fc1(x))
        # Apply ReLU activation function to the output of the second layer
        x = torch.relu(self.fc2(x))
        # Pass through the third layer to get the final output
        x = self.fc3(x)
        return x

# Define the ReplayMemory class to store experiences
class ReplayMemory:
    def __init__(self, capacity):
        # Use deque to store experiences with a maximum length
        self.memory = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        # Append a new experience to the memory
        self.memory.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        # Sample a batch of experiences from the memory
        return random.sample(self.memory, batch_size)

    def __len__(self):
        # Return the current size of the memory
        return len(self.memory)

# Define the DQNAgent class
class DQNAgent:
    def __init__(self, state_dim, action_dim):
        self.state_dim = state_dim
        self.action_dim = action_dim
        # Initialize ReplayMemory with a capacity of 100,000
        self.memory = ReplayMemory(100000)
        self.gamma = 0.99  # Discount factor
        self.epsilon = 0.2  # Exploration factor
        self.batch_size = 32  # Batch size for training
        # Initialize policy network and target network
        self.policy_net = DQN(state_dim, action_dim)
        self.target_net = DQN(state_dim, action_dim)
        # Define optimizer and loss function
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-3)
        self.criterion = nn.MSELoss()
        # Update target network with policy network weights
        self.update_target_net()
        # List to store loss values for analysis
        self.losses = []

    def update_target_net(self):
        # Copy the weights from policy network to target network
        self.target_net.load_state_dict(self.policy_net.state_dict())

    def select_action(self, state):
        # Epsilon-greedy action selection
        if random.random() < self.epsilon:
            # Choose random action with probability epsilon
            return random.randrange(self.action_dim)
        # Choose the action with the highest Q-value with probability 1-epsilon
        with torch.no_grad():
            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
            return self.policy_net(state).argmax().item()

    def learn(self):
        # Don't update the network if there's not enough data
        if len(self.memory) < self.batch_size:
            return

        # Sample a batch from the memory
        batch = self.memory.sample(self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        # Convert batch data to tensors
        states = torch.tensor(states, dtype=torch.float32)
        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)
        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)
        next_states = torch.tensor(next_states, dtype=torch.float32)
        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)

        # Calculate current Q-values and target Q-values
        current_q_values = self.policy_net(states).gather(1, actions)
        next_q_values = self.target_net(next_states).max(1)[0].unsqueeze(1)
        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)

        # Compute the loss
        loss = self.criterion(current_q_values, target_q_values)
        # Zero the gradients, perform backpropagation, and update the weights
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Append the loss for tracking
        self.losses.append(loss.item())

    def remember(self, state, action, reward, next_state, done):
        # Store the experience in replay memory
        self.memory.push(state, action, reward, next_state, done)

# Create the Acrobot-v1 environment
env = gym.make('Acrobot-v1')

# Get the dimensions of the state and action space
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.n

# Initialize the DQNAgent with state and action dimensions
agent = DQNAgent(state_dim, action_dim)

# Define the number of episodes to run the training
num_episodes = 2500
# List to store the total rewards for each episode
returns = []

# Lists to store the mean rewards and losses for every 100 episodes
mean_rewards = []
mean_losses = []

# Loop over each episode
for episode in range(num_episodes):
    # Reset the environment to get the initial state
    state = env.reset()
    done = False
    total_reward = 0

    # Run the episode until it is done
    while not done:
        # Select an action using the agent
        action = agent.select_action(state)
        # Take a step in the environment using the selected action
        next_state, reward, done, _ = env.step(action)
        # Store the experience in replay memory
        agent.remember(state, action, reward, next_state, done)
        # Perform learning step
        agent.learn()
        # Update the current state to the next state
        state = next_state
        # Accumulate the reward
        total_reward += reward

        # If the episode is done, store the total reward
        if done:
            returns.append(total_reward)
            break

    # Every 100 episodes, calculate and print the mean reward and mean loss
    if (episode + 1) % 100 == 0:
        mean_rewards.append(np.mean(returns[-100:]))
        mean_losses.append(np.mean(agent.losses[-100:]))
        print(f"Episode: {episode + 1}, Mean Reward: {mean_rewards[-1]}, Mean Loss: {mean_losses[-1]}")

    # Every 100 episodes, update the target network
    if episode % 100 == 0:
        agent.update_target_net()

# Define x-axis values for plotting, representing episode numbers
x = np.linspace(1, 2500, 25)

# Create a figure for plotting with a specified size
plt.figure(figsize=(12, 5))

# Create the first subplot for mean rewards
plt.subplot(1, 2, 1)
# Plot mean rewards over episodes
plt.plot(x, mean_rewards, color='green')
# Set the title of the first subplot
plt.title('Mean Reward over Episodes')
# Label the x-axis
plt.xlabel('Episode #')
# Label the y-axis
plt.ylabel('Mean Reward')
# Enable grid for better readability
plt.grid()

# Create the second subplot for mean losses
plt.subplot(1, 2, 2)
# Plot mean losses over episodes
plt.plot(x, mean_losses, color='red')
# Set the title of the second subplot
plt.title('Mean Loss over Episodes')
# Label the x-axis
plt.xlabel('Episode #')
# Label the y-axis
plt.ylabel('Mean Loss')
# Enable grid for better readability
plt.grid()

# Display the plots
plt.show()

